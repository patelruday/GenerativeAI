{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOfSDJ/fAJSGyR5zo4n/TiV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/patelruday/GenerativeAI/blob/main/pdf_chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "YtHxv46xEaNV",
        "outputId": "b475dedc-4545-42d3-9877-2586d948559a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'OM NAMO BHAGVATE VASUDEVAY'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "\"OM NAMO BHAGVATE VASUDEVAY\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "import uuid\n",
        "import PyPDF2\n",
        "import google.generativeai as genai\n",
        "import faiss\n",
        "import tempfile\n",
        "from langchain.docstore.in_memory import InMemoryDocstore\n",
        "from langchain_groq import ChatGroq\n",
        "# Initialize embeddings, FAISS, and LLM\n",
        "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "# Set up the FAISS index (L2 similarity)\n",
        "dimension = 384  # Dimension for the 'all-MiniLM-L6-v2' embeddings\n",
        "index = faiss.IndexFlatL2(dimension)\n",
        "\n",
        "# Set up an in-memory docstore to keep track of documents\n",
        "docstore = InMemoryDocstore({})\n",
        "index_to_docstore_id = {}\n",
        "\n",
        "# Initialize the FAISS vector store\n",
        "faiss_index = FAISS(\n",
        "    index=index,\n",
        "    docstore=docstore,\n",
        "    index_to_docstore_id=index_to_docstore_id,\n",
        "    embedding_function=embedding_model.embed_query\n",
        ")\n",
        "\n",
        "# Configure the LLM\n",
        "llm=ChatGroq(\n",
        "    model=\"llama-3.1-70b-versatile\",\n",
        "    groq_api_key=\"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\",\n",
        "    temperature=0,\n",
        "    max_tokens=None,\n",
        "    timeout=None,\n",
        "    max_retries=2,\n",
        "    # other params...\n",
        ")\n",
        "\n",
        "# Function to load and split PDFs\n",
        "def load_and_split_pdf(file_path):\n",
        "    pdf_reader = PyPDF2.PdfReader(file_path)\n",
        "    text_chunks = []\n",
        "    for page_num in range(len(pdf_reader.pages)):\n",
        "        page_text = pdf_reader.pages[page_num].extract_text()\n",
        "        # Split text into smaller chunks for processing\n",
        "        if page_text:\n",
        "            chunks = [page_text[i:i + 500] for i in range(0, len(page_text), 500)]\n",
        "            text_chunks.extend(chunks)\n",
        "    return text_chunks\n",
        "\n",
        "# Function to embed documents\n",
        "# Function to embed documents\n",
        "def embed_documents(text_chunks):\n",
        "    for chunk in text_chunks:\n",
        "        # Generate a unique ID for each document\n",
        "        doc_id = str(uuid.uuid4())\n",
        "        # Add the chunk (text) to the FAISS index, including metadata as a dictionary\n",
        "        faiss_index.add_texts([chunk], [{\"doc_id\": doc_id}])\n",
        "\n",
        "# Function to handle chat queries\n",
        "def chat_with_pdfs(query):\n",
        "    query_embedding = embedding_model.embed_query(query)\n",
        "    results = faiss_index.similarity_search_by_vector(query_embedding, k=5)\n",
        "    context = \" \".join([result.page_content for result in results])\n",
        "    response = llm.invoke(context + \"\\n\" + query)\n",
        "    return response\n",
        "\n",
        "# Streamlit UI\n",
        "st.title(\"Chat with Your PDF Documents\")\n",
        "st.write(\"Upload multiple PDF documents and chat with them!\")\n",
        "\n",
        "uploaded_files = st.file_uploader(\"Upload PDFs\", type=\"pdf\", accept_multiple_files=True)\n",
        "\n",
        "if uploaded_files:\n",
        "    st.write(\"Processing PDFs...\")\n",
        "    for uploaded_file in uploaded_files:\n",
        "        with tempfile.NamedTemporaryFile(delete=False) as temp_file:\n",
        "            temp_file.write(uploaded_file.read())\n",
        "            text_chunks = load_and_split_pdf(temp_file.name)\n",
        "            embed_documents(text_chunks)\n",
        "    st.write(\"PDFs have been successfully processed. You can now ask questions!\")\n",
        "\n",
        "# Chat interface\n",
        "query = st.text_input(\"Ask a question about your PDFs:\")\n",
        "if query:\n",
        "    response = chat_with_pdfs(query)\n",
        "    st.write(\"Answer:\", response.content)\n"
      ],
      "metadata": {
        "id": "l6zmTooHEh9g"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}